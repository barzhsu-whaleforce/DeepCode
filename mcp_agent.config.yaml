$schema: ./schema/mcp-agent.config.schema.json
anthropic:
  default_model: claude-opus-4-5-20251101
default_search_server: bocha-mcp
document_segmentation:
  enabled: true
  size_threshold_chars: 50000
execution_engine: asyncio
google:
  default_model: gemini-3-pro-preview
llm_provider: openai
logger:
  level: info
  path_settings:
    path_pattern: logs/mcp-agent-{unique_id}.jsonl
    timestamp_format: '%Y%m%d_%H%M%S'
    unique_id: timestamp
  progress_display: false
  transports:
  - console
  - file
mcp:
  servers:
    bocha-mcp:
      args:
      - tools/bocha_search_server.py
      command: python3
      env:
        BOCHA_API_KEY: sk-8f76930d5b0342dfb7c33ab9524f11e0
        PYTHONPATH: .
    code-implementation:
      args:
      - tools/code_implementation_server.py
      command: python
      description: Paper code reproduction tool server - provides file operations,
        code execution, search and other functions
      env:
        PYTHONPATH: .
    code-reference-indexer:
      args:
      - tools/code_reference_indexer.py
      command: python
      description: Code reference indexer server - Provides intelligent code reference
        search from indexed repositories
      env:
        PYTHONPATH: .
    command-executor:
      args:
      - tools/command_executor.py
      command: python
      env:
        PYTHONPATH: .
    document-segmentation:
      args:
      - tools/document_segmentation_server.py
      command: python
      description: Document segmentation server - Provides intelligent document analysis
        and segmented reading to optimize token usage
      env:
        PYTHONPATH: .
    fetch:
      args:
      - mcp-server-fetch
      command: uvx
    file-downloader:
      args:
      - tools/pdf_downloader.py
      command: python
      env:
        PYTHONPATH: .
    filesystem:
      args:
      - -y
      - '@modelcontextprotocol/server-filesystem'
      - .
      command: npx
    github-downloader:
      args:
      - tools/git_command.py
      command: python
      env:
        PYTHONPATH: .
openai:
  base_max_tokens: 40000
  default_model: gpt-5.1-codex-max
  max_tokens_policy: adaptive
  reasoning_effort: medium
  retry_max_tokens: 8192
planning_mode: traditional

# Task-level model configuration for multi-model support
# This allows different tasks to use different models for cost/quality optimization
task_models:
  # Tier 1: Simple tasks - fast and cheap (Azure gpt-4o-mini)
  simple:
    provider: openai
    model: gpt-4o-mini
    api_type: chat
    tasks:
      - research_analyzer
      - resource_processor
      - github_download
      - structure_generator

  # Tier 2: Medium tasks - balanced (Google Gemini - free tier)
  medium:
    provider: google
    model: gemini-3-pro-preview
    tasks:
      - concept_analysis
      - reference_analysis

  # Tier 3: Complex tasks - high quality (Azure codex)
  complex:
    provider: openai
    model: gpt-5.1-codex-max
    api_type: responses
    tasks:
      - algorithm_analysis
      - code_planning
      - code_implementation
      - chat_planning

  # Summary tasks - fast and free (Google Flash)
  summary:
    provider: google
    model: gemini-2.0-flash
    tasks:
      - memory_summary
